{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cadre de la reconnaissance d'image, l'image est découpée en petites zones (appelées tuiles). Chaque tuile sera traitée individuellement par un neurone artificiel : il effectue une opération de filtrage en associant un poids à chaque pixel de la tuile. Tous les neurones ont les mêmes paramètres de réglage. Le fait d'avoir le même traitement (mêmes paramètres), légèrement décalé pour chaque champ récepteur, s'appelle une convolution. Cette strate de neurones avec les mêmes paramètres est appelée « noyau de convolution ».\n",
    "\n",
    "Les pixels d'une tuile sont analysés globalement. Dans le cas d'une image en couleur, un pixel contient 3 entrées (rouge, vert et bleu), qui seront traitées globalement par chaque neurone. Donc l'image peut être considérée comme un volume, et notée par exemple 30 × 10 × 3 pour 30 pixels de largeur, 10 de hauteur et 3 de profondeur correspondant aux 3 canaux rouge, vert et bleu. De manière générale, on parlera de « volume d'entrée ».\n",
    "\n",
    "Dans les faits, la zone analysée est légèrement plus grande que la tuile et est appelée « champ récepteur ». Les champs récepteurs se chevauchent donc, afin d'obtenir une meilleure représentation de l'image originale ainsi qu'une meilleure cohérence du traitement au fil des couches de traitement. Le chevauchement est défini par le pas (décalage entre deux champs récepteurs adjacents).\n",
    "\n",
    "Un noyau de convolution va analyser une caractéristique de l'image d'entrée. Pour analyser plusieurs caractéristiques, on  empile des strates de noyaux de convolution indépendants, chaque strate analysant une caractéristique de l'image. L'ensemble des strates ainsi empilées forme la « couche de traitement convolutif », qu'il faut voir en fait comme un volume (souvent appelé « volume de sortie »). Le nombre de strates de traitement s'appelle la profondeur de la couche de convolution.\n",
    "\n",
    "Une couche de convolution permet de traiter un volume d'entrée pour fournir un volume de sortie. On peut également assimiler le volume de sortie à une image intermédiaire.\n",
    "\n",
    "Le problème est divisé en sous parties, et pour chaque partie, un «cluster» de neurones sera créer afin\n",
    "d’étudier cette portion spécifique. Par exemple, pour une image en couleur, il est possible de\n",
    "diviser l’image sur la largeur, la hauteur et la profondeur (les couleurs). \n",
    "\n",
    "Une architecture CNN est formée par un empilement de couches de traitement indépendantes :\n",
    "La couche de convolution (CONV) qui traite les données d'un champ récepteur.\n",
    "        La couche de convolution est le bloc de construction de base d'un CNN. Trois paramètres permettent de\n",
    "        dimensionner le volume de la couche de convolution la profondeur (nombre de noyau de convolution), le pas et la marge.\n",
    "La couche de pooling (POOL), qui permet de compresser l'information en réduisant la taille de\n",
    "l'image intermédiaire (souvent par sous-échantillonnage).\n",
    "        Un autre concept important des CNNs est le pooling, ce qui est une forme de sous-échantillonnage de\n",
    "        l'image. L'image d'entrée est découpée en une série de rectangles de n pixels de côté ne se chevauchant\n",
    "        pas (pooling). Chaque rectangle peut être vu comme une tuile. Le signal en sortie de tuile est défini en\n",
    "        fonction des valeurs prises par les différents pixels de la tuile.\n",
    "La couche de correction (ReLU), souvent appelée par abus 'ReLU' en référence à la fonction\n",
    "d'activation (Unité de rectification linéaire).\n",
    "        Il est possible d'améliorer l'efficacité du traitement en intercalant entre les couches de traitement une\n",
    "        couche qui va opérer une fonction mathématique (fonction d'activation) sur les signaux de sortie.\n",
    "La couche \"entièrement connectée\" (FC), qui est une couche de type perceptron.\n",
    "        Après plusieurs couches de convolution et de max-pooling, le raisonnement de haut niveau dans le réseau\n",
    "        neuronal se fait via des couches entièrement connectées. Les neurones dans une couche entièrement\n",
    "        connectée ont des connexions vers toutes les sorties de la couche précédente. Leurs fonctions\n",
    "        d'activations peuvent donc être calculées avec une multiplication matricielle suivie d'un décalage de\n",
    "        polarisation\n",
    "La couche de perte (LOSS).\n",
    "        La couche de perte spécifie comment l'entrainement du réseau pénalise l'écart entre le signal prévu et réel. \n",
    "\n",
    "\n",
    "\n",
    "Pour classifier les images à partir de la bibliothèque keras\n",
    "CNN1 : \n",
    "- temps de latence par epoch : 16s\n",
    "- perte :  267\n",
    "- precision : 0.68%\n",
    "\n",
    "Entraînement : \n",
    "\n",
    "\n",
    "code:\n",
    "CNN1 = keras.Sequential()\n",
    "CNN1.add(keras.layers.Conv2D(a, kernel_size=(x,y), activation=b, strides=(c,d)))\n",
    "CNN1.add(keras.layers.Flatten())\n",
    "CNN1.add(keras.layers.Dense(64, activation=b))\n",
    "CNN1.add(keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "CNN1.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss='sparse_categorical_crossentropy',metrics = 'accuracy')\n",
    "CNN1.fit(x_train, y_train, epochs=10, batch_size=100)\n",
    "\n",
    "CNN2 : \n",
    "- temps de latence par epoch : 18s\n",
    "- perte : 8735\n",
    "- precision : 0.40%\n",
    "\n",
    "Entraînement : \n",
    "\n",
    "\n",
    "code:\n",
    "CNN2 = keras.Sequential()\n",
    "CNN2.add(keras.layers.Conv2D(a, kernel_size=(x,y), activation=b, strides=(c,d)))\n",
    "CNN2.add(keras.layers.BatchNormalization())\n",
    "CNN2.add(keras.layers.Flatten())\n",
    "CNN2.add(keras.layers.Dense(64, activation=b))\n",
    "CNN2.add(keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "CNN2.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss='sparse_categorical_crossentropy',metrics = 'accuracy')\n",
    " \n",
    "CNN2.fit(x_train, y_train, epochs=10, batch_size=100)\n",
    "\n",
    "CNN3 : \n",
    "- temps de latence par epoch : 18s\n",
    "- perte : 349\n",
    "- precision : 0.64%\n",
    "\n",
    "Entraînement : \n",
    "\n",
    "\n",
    "code:\n",
    "CNN3 = keras.Sequential()\n",
    "CNN3.add(keras.layers.Conv2D(2a, kernel_size=(x,y), activation=b, strides=(c,d)))\n",
    "CNN3.add(keras.layers.MaxPooling2D(2,2))\n",
    "CNN3.add(keras.layers.Dropout(0.25))\n",
    "CNN3.add(keras.layers.Flatten())\n",
    "CNN3.add(keras.layers.Dense(64, activation=b))\n",
    "CNN3.add(keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "CNN3.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss='sparse_categorical_crossentropy',metrics = 'accuracy')\n",
    " \n",
    "CNN3.fit(x_train, y_train, epochs=10, batch_size=100)\n",
    "\n",
    "CNN4 : \n",
    "- temps de latence par epoch : 11s\n",
    "- perte : 251\n",
    "- precision : 0.72%\n",
    "\n",
    "Entraînement : \n",
    "\n",
    "code:\n",
    "CNN4 = keras.Sequential()\n",
    "CNN4.add(keras.layers.Conv2D(a, kernel_size=(x,y), activation=b, strides=(c,d)))\n",
    "CNN4.add(keras.layers.MaxPooling2D(2,2))\n",
    "CNN4.add(keras.layers.Conv2D(a, kernel_size=(x,y), activation=b, strides=(c,d)))\n",
    "CNN4.add(keras.layers.MaxPooling2D(2,2))\n",
    "CNN4.add(keras.layers.Dropout(0.25))\n",
    "CNN4.add(keras.layers.Flatten())\n",
    "CNN4.add(keras.layers.Dense(64, activation=b))\n",
    "CNN4.add(keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "CNN4.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss='sparse_categorical_crossentropy',metrics = 'accuracy')\n",
    " \n",
    "CNN4.fit(x_train, y_train, epochs=10, batch_size=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
